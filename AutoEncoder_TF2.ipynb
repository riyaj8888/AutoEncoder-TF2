{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoEncoder_TF2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu22ganca2eb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX4yHUOZbU1n"
      },
      "source": [
        "class AE(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,num_of_encoder_block=None,num_of_decoder_block=None,num_of_base_filters=None,c_factor = 0.5):\n",
        "\n",
        "    super(AE,self).__init__()\n",
        "\n",
        "    self.num_of_encoder_block = num_of_encoder_block\n",
        "    self.num_of_decoder_block = num_of_decoder_block\n",
        "\n",
        "    self.enc_filters = num_of_base_filters*np.arange(self.num_of_encoder_block )\n",
        "    self.dec_filters = self.enc_filters[::-1]*c_factor\n",
        "\n",
        "\n",
        "    self.encode_convs  = [Conv2D(filters=self.enc_filters[h]+num_of_base_filters,kernel_size=(3,3),strides=(1,1),padding='same',name = 'en_conv_'+ str(h)) for h in range(self.num_of_encoder_block)]\n",
        "    self.encode_bn  = [BatchNormalization(name = 'en_bn_'+ str(h)) for h in range(self.num_of_encoder_block)]\n",
        "    self.encode_activation  = [ReLU(name = 'en_relu_'+ str(h)) for h in range(self.num_of_encoder_block)]\n",
        "\n",
        "    self.decode_convs  = [Conv2D(filters=self.dec_filters[h]+num_of_base_filters//2,kernel_size=(3,3),strides=(1,1),padding='same',name='dec_conv_'+ str(h)) for h in range(self.num_of_decoder_block)]\n",
        "    self.decode_bn  = [BatchNormalization(name='dec_bn_'+ str(h)) for h in range(self.num_of_decoder_block)]\n",
        "    self.decode_activation  = [ReLU(name='dec_relu_'+ str(h)) for h in range(self.num_of_decoder_block)]\n",
        "\n",
        "    self.final_conv = Conv2D(1,1,padding='same')\n",
        "    self.final_act = tf.keras.activations.sigmoid\n",
        "\n",
        "  def encode_info(self,x):\n",
        "\n",
        "    for l in range(self.num_of_encoder_block):\n",
        "\n",
        "      x = self.encode_convs[l](x)\n",
        "      x = self.encode_bn[l](x)\n",
        "      x = self.encode_activation[l](x)\n",
        "\n",
        "      if l % 2 == 0:\n",
        "        x = MaxPool2D()(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def decode_info(self,x):\n",
        "\n",
        "    for l in range(self.num_of_decoder_block):\n",
        "\n",
        "      x = self.decode_convs[l](x)\n",
        "      x = self.decode_bn[l](x)\n",
        "      x = self.decode_activation[l](x)\n",
        "\n",
        "      if l % 2 == 0:\n",
        "\n",
        "        x = Conv2DTranspose(filters = self.dec_filters[l]+num_of_base_filters//2,kernel_size=(3,3), strides=2,padding='same')(x)\n",
        "        x = ReLU()(x)\n",
        "\n",
        "      if l == self.num_of_decoder_block-1:\n",
        "        x = self.final_conv(x)\n",
        "        x = self.final_act(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def call(self,x):\n",
        "\n",
        "    x = self.encode_info(x)\n",
        "    x = self.decode_info(x)\n",
        "\n",
        "\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPI0MmbBfOWv",
        "outputId": "df020270-a852-4aa5-acb4-c3ead047e0d3"
      },
      "source": [
        "x = Input(shape=(28,28,1))\n",
        "\n",
        "num_of_encoder_block = 3\n",
        "num_of_base_filters = 64\n",
        "num_of_decoder_block = 3\n",
        "\n",
        "AE_Class = AE(num_of_encoder_block=num_of_encoder_block,num_of_decoder_block=num_of_decoder_block,num_of_base_filters=num_of_base_filters)\n",
        "\n",
        "e1 = AE_Class.encode_info(x)\n",
        "d1 = AE_Class.decode_info(e1)\n",
        "\n",
        "\n",
        "encoder_model = tf.keras.models.Model(x,e1)\n",
        "encoder_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "AE_model = tf.keras.models.Model(inputs = x,outputs=d1)\n",
        "AE_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_17 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "en_conv_0 (Conv2D)           (None, 28, 28, 64)        640       \n",
            "_________________________________________________________________\n",
            "en_bn_0 (BatchNormalization) (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "en_relu_0 (ReLU)             (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "en_conv_1 (Conv2D)           (None, 14, 14, 128)       73856     \n",
            "_________________________________________________________________\n",
            "en_bn_1 (BatchNormalization) (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "en_relu_1 (ReLU)             (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "en_conv_2 (Conv2D)           (None, 14, 14, 192)       221376    \n",
            "_________________________________________________________________\n",
            "en_bn_2 (BatchNormalization) (None, 14, 14, 192)       768       \n",
            "_________________________________________________________________\n",
            "en_relu_2 (ReLU)             (None, 14, 14, 192)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_41 (MaxPooling (None, 7, 7, 192)         0         \n",
            "=================================================================\n",
            "Total params: 297,408\n",
            "Trainable params: 296,640\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "Model: \"model_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_17 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "en_conv_0 (Conv2D)           (None, 28, 28, 64)        640       \n",
            "_________________________________________________________________\n",
            "en_bn_0 (BatchNormalization) (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "en_relu_0 (ReLU)             (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_40 (MaxPooling (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "en_conv_1 (Conv2D)           (None, 14, 14, 128)       73856     \n",
            "_________________________________________________________________\n",
            "en_bn_1 (BatchNormalization) (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "en_relu_1 (ReLU)             (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "en_conv_2 (Conv2D)           (None, 14, 14, 192)       221376    \n",
            "_________________________________________________________________\n",
            "en_bn_2 (BatchNormalization) (None, 14, 14, 192)       768       \n",
            "_________________________________________________________________\n",
            "en_relu_2 (ReLU)             (None, 14, 14, 192)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_41 (MaxPooling (None, 7, 7, 192)         0         \n",
            "_________________________________________________________________\n",
            "dec_conv_0 (Conv2D)          (None, 7, 7, 96)          165984    \n",
            "_________________________________________________________________\n",
            "dec_bn_0 (BatchNormalization (None, 7, 7, 96)          384       \n",
            "_________________________________________________________________\n",
            "dec_relu_0 (ReLU)            (None, 7, 7, 96)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_35 (Conv2DT (None, 14, 14, 96)        83040     \n",
            "_________________________________________________________________\n",
            "re_lu_35 (ReLU)              (None, 14, 14, 96)        0         \n",
            "_________________________________________________________________\n",
            "dec_conv_1 (Conv2D)          (None, 14, 14, 64)        55360     \n",
            "_________________________________________________________________\n",
            "dec_bn_1 (BatchNormalization (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "dec_relu_1 (ReLU)            (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dec_conv_2 (Conv2D)          (None, 14, 14, 32)        18464     \n",
            "_________________________________________________________________\n",
            "dec_bn_2 (BatchNormalization (None, 14, 14, 32)        128       \n",
            "_________________________________________________________________\n",
            "dec_relu_2 (ReLU)            (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_36 (Conv2DT (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "re_lu_36 (ReLU)              (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 28, 28, 1)         33        \n",
            "_________________________________________________________________\n",
            "tf.math.sigmoid_14 (TFOpLamb (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 630,305\n",
            "Trainable params: 629,153\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWZtMoEwjk3k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOsxC9L30DxY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H9DH40d19KS",
        "outputId": "d48ba3b1-94c4-4420-c29f-302c3ef5d2cf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder weights are set !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKZvCp2gytaV"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1,28,28,1)\n",
        "x_test = x_test.reshape(-1,28,28,1)\n",
        "# assert x_train.shape == (60000, 28, 28)\n",
        "# assert x_test.shape == (10000, 28, 28)\n",
        "# assert y_train.shape == (60000,)\n",
        "# assert y_test.shape == (10000,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dopm4eCM7LZ8"
      },
      "source": [
        "AE_model.compile(\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=1e-3),  # Optimizer\n",
        "    # Loss function to minimize\n",
        "    loss = tf.keras.losses.MeanSquaredError(),\n",
        "\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yALYcoAf7koj",
        "outputId": "5aa9d4ac-5107-411c-9200-1cbe63ed76e1"
      },
      "source": [
        "history = AE_model.fit(\n",
        "    x_train/255.,\n",
        "    x_train/255,\n",
        "    batch_size=64,\n",
        "    epochs=200,\n",
        "    # We pass some validation for\n",
        "    # monitoring validation loss and metrics\n",
        "    # at the end of each epoch\n",
        "    validation_split=0.3\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0135 - val_loss: 0.0130\n",
            "Epoch 2/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0133 - val_loss: 0.0128\n",
            "Epoch 3/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0131 - val_loss: 0.0126\n",
            "Epoch 4/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0129 - val_loss: 0.0125\n",
            "Epoch 5/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0128 - val_loss: 0.0123\n",
            "Epoch 6/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0126 - val_loss: 0.0122\n",
            "Epoch 7/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0125 - val_loss: 0.0120\n",
            "Epoch 8/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0123 - val_loss: 0.0119\n",
            "Epoch 9/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0122 - val_loss: 0.0117\n",
            "Epoch 10/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0121 - val_loss: 0.0116\n",
            "Epoch 11/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0119 - val_loss: 0.0115\n",
            "Epoch 12/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0118 - val_loss: 0.0113\n",
            "Epoch 13/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0117 - val_loss: 0.0112\n",
            "Epoch 14/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0115 - val_loss: 0.0111\n",
            "Epoch 15/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0114 - val_loss: 0.0110\n",
            "Epoch 16/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0113 - val_loss: 0.0109\n",
            "Epoch 17/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0112 - val_loss: 0.0108\n",
            "Epoch 18/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0111 - val_loss: 0.0107\n",
            "Epoch 19/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0110 - val_loss: 0.0106\n",
            "Epoch 20/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0109 - val_loss: 0.0105\n",
            "Epoch 21/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0108 - val_loss: 0.0104\n",
            "Epoch 22/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0107 - val_loss: 0.0103\n",
            "Epoch 23/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0106 - val_loss: 0.0102\n",
            "Epoch 24/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0105 - val_loss: 0.0101\n",
            "Epoch 25/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0104 - val_loss: 0.0100\n",
            "Epoch 26/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0103 - val_loss: 0.0099\n",
            "Epoch 27/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0103 - val_loss: 0.0098\n",
            "Epoch 28/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0102 - val_loss: 0.0098\n",
            "Epoch 29/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0101 - val_loss: 0.0097\n",
            "Epoch 30/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0100 - val_loss: 0.0096\n",
            "Epoch 31/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0099 - val_loss: 0.0095\n",
            "Epoch 32/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0099 - val_loss: 0.0095\n",
            "Epoch 33/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0098 - val_loss: 0.0094\n",
            "Epoch 34/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0097 - val_loss: 0.0093\n",
            "Epoch 35/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0097 - val_loss: 0.0093\n",
            "Epoch 36/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0096 - val_loss: 0.0092\n",
            "Epoch 37/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0095 - val_loss: 0.0091\n",
            "Epoch 38/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0095 - val_loss: 0.0091\n",
            "Epoch 39/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0094 - val_loss: 0.0090\n",
            "Epoch 40/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0093 - val_loss: 0.0089\n",
            "Epoch 41/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0093 - val_loss: 0.0089\n",
            "Epoch 42/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0092 - val_loss: 0.0088\n",
            "Epoch 43/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0092 - val_loss: 0.0088\n",
            "Epoch 44/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0091 - val_loss: 0.0087\n",
            "Epoch 45/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0090 - val_loss: 0.0087\n",
            "Epoch 46/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0090 - val_loss: 0.0086\n",
            "Epoch 47/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0089 - val_loss: 0.0086\n",
            "Epoch 48/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0089 - val_loss: 0.0085\n",
            "Epoch 49/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0088 - val_loss: 0.0085\n",
            "Epoch 50/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0088 - val_loss: 0.0084\n",
            "Epoch 51/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0087 - val_loss: 0.0084\n",
            "Epoch 52/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0087 - val_loss: 0.0083\n",
            "Epoch 53/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0086 - val_loss: 0.0083\n",
            "Epoch 54/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0086 - val_loss: 0.0082\n",
            "Epoch 55/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0085 - val_loss: 0.0082\n",
            "Epoch 56/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0085 - val_loss: 0.0081\n",
            "Epoch 57/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0085 - val_loss: 0.0081\n",
            "Epoch 58/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0084 - val_loss: 0.0081\n",
            "Epoch 59/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0084 - val_loss: 0.0080\n",
            "Epoch 60/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0083 - val_loss: 0.0080\n",
            "Epoch 61/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0083 - val_loss: 0.0079\n",
            "Epoch 62/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0082 - val_loss: 0.0079\n",
            "Epoch 63/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0082 - val_loss: 0.0078\n",
            "Epoch 64/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0082 - val_loss: 0.0078\n",
            "Epoch 65/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0081 - val_loss: 0.0078\n",
            "Epoch 66/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0081 - val_loss: 0.0077\n",
            "Epoch 67/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0080 - val_loss: 0.0077\n",
            "Epoch 68/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0080 - val_loss: 0.0076\n",
            "Epoch 69/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0080 - val_loss: 0.0076\n",
            "Epoch 70/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0079 - val_loss: 0.0076\n",
            "Epoch 71/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0079 - val_loss: 0.0076\n",
            "Epoch 72/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0079 - val_loss: 0.0075\n",
            "Epoch 73/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0078 - val_loss: 0.0075\n",
            "Epoch 74/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0078 - val_loss: 0.0074\n",
            "Epoch 75/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0078 - val_loss: 0.0074\n",
            "Epoch 76/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0077 - val_loss: 0.0074\n",
            "Epoch 77/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0077 - val_loss: 0.0074\n",
            "Epoch 78/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0077 - val_loss: 0.0073\n",
            "Epoch 79/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0076 - val_loss: 0.0073\n",
            "Epoch 80/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0076 - val_loss: 0.0073\n",
            "Epoch 81/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0076 - val_loss: 0.0072\n",
            "Epoch 82/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0075 - val_loss: 0.0072\n",
            "Epoch 83/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0075 - val_loss: 0.0072\n",
            "Epoch 84/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0075 - val_loss: 0.0072\n",
            "Epoch 85/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0075 - val_loss: 0.0071\n",
            "Epoch 86/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0074 - val_loss: 0.0071\n",
            "Epoch 87/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0074 - val_loss: 0.0071\n",
            "Epoch 88/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0074 - val_loss: 0.0070\n",
            "Epoch 89/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0074 - val_loss: 0.0070\n",
            "Epoch 90/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0073 - val_loss: 0.0070\n",
            "Epoch 91/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0073 - val_loss: 0.0070\n",
            "Epoch 92/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0073 - val_loss: 0.0069\n",
            "Epoch 93/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0072 - val_loss: 0.0069\n",
            "Epoch 94/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0072 - val_loss: 0.0069\n",
            "Epoch 95/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0072 - val_loss: 0.0069\n",
            "Epoch 96/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0072 - val_loss: 0.0068\n",
            "Epoch 97/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0071 - val_loss: 0.0068\n",
            "Epoch 98/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0071 - val_loss: 0.0068\n",
            "Epoch 99/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0071 - val_loss: 0.0068\n",
            "Epoch 100/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0071 - val_loss: 0.0067\n",
            "Epoch 101/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0070 - val_loss: 0.0067\n",
            "Epoch 102/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0070 - val_loss: 0.0067\n",
            "Epoch 103/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0070 - val_loss: 0.0067\n",
            "Epoch 104/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0070 - val_loss: 0.0067\n",
            "Epoch 105/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0070 - val_loss: 0.0066\n",
            "Epoch 106/200\n",
            "657/657 [==============================] - 10s 16ms/step - loss: 0.0069 - val_loss: 0.0066\n",
            "Epoch 107/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0069 - val_loss: 0.0066\n",
            "Epoch 108/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0069 - val_loss: 0.0066\n",
            "Epoch 109/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0069 - val_loss: 0.0065\n",
            "Epoch 110/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0068 - val_loss: 0.0065\n",
            "Epoch 111/200\n",
            "657/657 [==============================] - 11s 16ms/step - loss: 0.0068 - val_loss: 0.0065\n",
            "Epoch 112/200\n",
            "229/657 [=========>....................] - ETA: 6s - loss: 0.0068"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-495eda897f61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# monitoring validation loss and metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# at the end of each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGY-eFTDDWDh"
      },
      "source": [
        "def enc_feature_layer_idx(AE_model,num_of_encoder_block):\n",
        "\n",
        "\n",
        "  for l,layer in enumerate(AE_model.layers):\n",
        "\n",
        "    if layer.name == 'en_relu_'+str(num_of_encoder_block-1):\n",
        "\n",
        "      en_idx = l\n",
        "  \n",
        "  return en_idx\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kls56TljDY04"
      },
      "source": [
        "def set_encoder_weights(AE_model,encoder_model,num_of_encoder_block):\n",
        "\n",
        "  en_idx = enc_feature_layer_idx(AE_model,num_of_encoder_block)\n",
        "\n",
        "  i=0\n",
        "\n",
        "  for l1,l2 in (zip(AE_model.layers,encoder_model.layers)):\n",
        "\n",
        "    if i < en_idx+1:\n",
        "\n",
        "      w = l1.get_weights()\n",
        "\n",
        "      l2.set_weights(w)\n",
        "\n",
        "      i=i+1\n",
        "\n",
        "  print(\"encoder weights are set !\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpDJgnKeDbmT",
        "outputId": "0d861ec9-787f-4e3a-ba9b-c2b0db6537ad"
      },
      "source": [
        "set_encoder_weights(AE_model,encoder_model,3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder weights are set !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWelu6WZMoz-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF5UWT09EOOq",
        "outputId": "2f23682e-8e12-4916-e191-903d60488569"
      },
      "source": [
        "x_test[0].shape\n",
        "x = x_test[0].reshape(1,28,28,1)\n",
        "x.shape\n",
        "encoder_model(x).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 7, 7, 192])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "7i5wXLJdECFI",
        "outputId": "6889ee58-c195-4d76-ac17-7c041238f156"
      },
      "source": [
        "y = AE_model.predict(x/255.).reshape(28,28)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(y)\n",
        "plt.show()\n",
        "plt.figure()\n",
        "plt.imshow(x.reshape(28,28))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}